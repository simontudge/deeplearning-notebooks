{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demonstartion I show how Conv Nets can be used to categorise images. Look at the notebook entiteled \"image-scrapper.ipynb\". This uses the bing API to search the web for an image. It finds around 1000 images for a search term, such as 'cat', and saves them in a directory of the same name. It turns them into black and white images for simplicity and numbers them all cat_1.jpg etc. The order therefore may be somewhat significant as they are in the same order as bing gives them to us.\n",
    "\n",
    "This notebook tries to create a catagorical classifier for each of the given categories.\n",
    "\n",
    "We can get moderate accuracy, of around 60% with 5 categories. There is definite scope for improvement, but the data set is quite small and a bit messy, so this might dictate an upper limit of how well any model can do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Take each image and convert it to a numpy array.\n",
    "* Then make an deep array of images of the shape (datasize, imagesize, imagesize)\n",
    "* We also need to make a similiar array of labels, these are simply integers to begin with.\n",
    "* One hot encode the integers (one for each class).\n",
    "* There will be a small number of classes (typically around 5, or even only 2)\n",
    "* Split the data into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import glob, os\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "import seaborn\n",
    "\n",
    "# Only use this for splitting the arrays\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def file_to_numpy(file_name):\n",
    "    \"\"\"\n",
    "    Return a numpy array from a filename.\n",
    "    \"\"\"\n",
    "    \n",
    "    im = Image.open(file_name)\n",
    "    arr = np.array(im.getdata())\n",
    "    # Assuming the image is square\n",
    "    size = int(np.sqrt(arr.shape[0]))\n",
    "    arr = arr.reshape((size,size,1))\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_file(file_name):\n",
    "    \"\"\"\n",
    "    Santity check function, simply looks at the data and makes\n",
    "    sure it still makes sense by plotting it.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = file_to_numpy(file_name)\n",
    "    imshow(data[:,:,0], interpolation='nearest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_file = '../data/convnet/cat/cat_4.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_file(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still looks like a cat, that's a good start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the categories here. Create lookups of integers to the categories, and visa-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories = ['cat', 'donkey', 'monkey', 'donald_trump', 'dog']\n",
    "total_categories = len(categories)\n",
    "num2cat = {i:c for i,c in enumerate(categories)}\n",
    "cat2num = {c:i for i,c in enumerate(categories)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For larger data sets it would be sensible to do something more clever here with batching the data, but as we don't have that much it doesn't matter too much here, so load the whole thing into memory. We also exclude any images that are literally identical, of which there are a few. This does not do anything about images that are almost the same though, which might be something to think about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "base_dir = \"../data/convnet\"\n",
    "for c in categories:\n",
    "    c_dir = os.path.join(base_dir, c)\n",
    "    glob_str = os.path.join(c_dir, \"{}*.jpg\".format(c))\n",
    "    print(\"{}:{}\".format(c, len(glob.glob(glob_str))))\n",
    "    # Keep a set of pictures that we have seen, so that\n",
    "    # we can reject duplicates. This assumes that there\n",
    "    # are only duplicates within the same directory\n",
    "    all_pics = set()\n",
    "    total_duplicates = 0\n",
    "    for f in glob.glob(glob_str):\n",
    "        array = file_to_numpy(f)\n",
    "        HASH = hash(array.tostring())\n",
    "        if HASH in all_pics:\n",
    "            total_duplicates += 1\n",
    "            continue\n",
    "        else:\n",
    "            all_pics.add(HASH)\n",
    "        data.append(array)\n",
    "        labels.append(cat2num[c])\n",
    "    print(\"Total duplicates: {}\".format(total_duplicates))\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "labels = keras.utils.to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning will work better if the data is centred around zero, and has a standard deviation of around 1. Let's perform this rescale here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data - data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data/data.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and testing data. Note that this also does the shuffeling for us, as our data is in a non-random order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, train_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the data through one convoulution step, a pooling step, one hidden layer and then the output layer. I've played aroung with a fair few model settings and this one seems to be the best. Much bigger models don't have enough data to train, although I find that any sensible choice of model does roughly the same, suggesting that the size and quality of the data may be the thing that is limiting the accuracy, not the model choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "depth = 16\n",
    "hidden_nodes = 64\n",
    "kernel_size = 5\n",
    "pool_size = 2\n",
    "dropout_rate = 0.5\n",
    "image_size = data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(input_shape=(image_size, image_size, 1), filters=depth, kernel_size=kernel_size))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=(pool_size, pool_size)))\n",
    "# model.add(keras.layers.Conv2D(filters=depth*2, kernel_size=kernel_size))\n",
    "# model.add(keras.layers.MaxPool2D(pool_size=(pool_size, pool_size)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(hidden_nodes, activation='relu'))\n",
    "model.add(keras.layers.Dropout(dropout_rate))\n",
    "# model.add(keras.layers.Dense(int(hidden_nodes/2), activation='relu'))\n",
    "model.add(keras.layers.Dense(total_categories, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply need to train this model now, which is just a built in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(train_data, train_labels, batch_size=128, epochs=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a few examples in an explict way, and compute the accuracy and the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pretty_predict(index):\n",
    "    \"\"\"\n",
    "    Predicts a single input with a picture and a verbose output.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Index: {}\".format(index))\n",
    "    label = num2cat[train_labels[index].argmax()]\n",
    "    print(\"It's a {}\".format(label))\n",
    "    data = train_data[index]\n",
    "    imshow(data[:,:,0], interpolation='nearest')\n",
    "    prediction = model.predict(np.array([data]))\n",
    "    predicted_label = num2cat[prediction.argmax()]\n",
    "    print(\"Model says that this is a {}\".format(predicted_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pretty_predict(np.random.randint(len(train_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final illustration I plot the confusion matrix. This shows how often class i is predicted as class j. Large diagonal elements are the ones the model gets right. This can often give some clues as the where the model is going wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data)\n",
    "predictions = [num2cat[p.argmax()] for p in predictions]\n",
    "labels = [num2cat[p.argmax()] for p in test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(9,9))\n",
    "ax = seaborn.heatmap(confusion_matrix(predictions, labels, labels=categories),\n",
    "                     xticklabels=categories, yticklabels=categories,\n",
    "                     annot=True, cbar=False, square=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
